{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AB: I am using parts of a notebook from the \"Python Data Science Handbook\" by Jake VanderPlas. \n",
    "\n",
    "I really like his material, examples, and notebooks. All his notebooks are available in github and Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5vEB398r820"
   },
   "source": [
    "\n",
    "\n",
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIDVIA0Yr82-"
   },
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "v-mIP879r83B"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiaGkID4r83E"
   },
   "source": [
    "## Introducing Principal Component Analysis\n",
    "\n",
    "Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data.\n",
    "\n",
    "Its behavior is easiest to visualize by looking at a two-dimensional dataset.\n",
    "Consider the following 200 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "sDyw5pYBr83F",
    "outputId": "808db8fb-5dc4-4877-8fa5-72249cb8e992"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pdOE_9qNr83M",
    "outputId": "8b11c760-839c-41b9-c696-e23b85fa10bb"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUbu13e3r83N"
   },
   "source": [
    "The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wAqph3vPr83N",
    "outputId": "d7901d30-b6df-4d83-c7cc-2d3e9357f815"
   },
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Dfx7ULxur83O",
    "outputId": "c7f2c66b-664b-40e7-c684-dd1f00632ad1"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ghi4-Car83P"
   },
   "source": [
    "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "GEd_KVffr83P",
    "outputId": "01030bb8-3671-4fc2-b650-60d4229df3e9"
   },
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    color = 'black',\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCNVa5Zsr83P"
   },
   "source": [
    "These vectors represent the *principal axes* of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis.\n",
    "The projection of each data point onto the principal axes are the \"principal components\" of the data.\n",
    "\n",
    "If we plot these principal components beside the original data, we see the plots shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxU3sHBzr83Q"
   },
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.09-PCA-rotation.png?raw=1)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Principal-Components-Rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgTCuKYur83Q"
   },
   "source": [
    "This transformation from data axes to principal axes is an *affine transformation*, which basically means it is composed of a translation, rotation, and uniform scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIzGQuF0r83Q"
   },
   "source": [
    "### PCA as dimensionality reduction\n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
    "\n",
    "Here is an example of using PCA as a dimensionality reduction transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fXOB0tTXr83R",
    "outputId": "7a6e4d7e-0d31-49ce-de6c-70a1b744e5ee"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZQbFkRnr83S"
   },
   "source": [
    "The transformed data has been reduced to a single dimension.\n",
    "To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "y1Kect9Gr83T",
    "outputId": "dd401916-f93a-44e7-f32b-75098f283f0d"
   },
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhPz6Td5r83T"
   },
   "source": [
    "The light points are the original data, while the dark points are the projected version.\n",
    "This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\n",
    "The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
    "\n",
    "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHPtzgVRr83T"
   },
   "source": [
    "### PCA for visualization: Hand-written digits\n",
    "\n",
    "The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data.\n",
    "To see this, let's take a quick look at the application of PCA to the digits data we saw in [In-Depth: Decision Trees and Random Forests](05.08-Random-Forests.ipynb).\n",
    "\n",
    "We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ptlhX54Lr83U",
    "outputId": "2a369928-3ff6-4f30-88eb-44cca6bcbe51"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zerbLyo1r83U"
   },
   "source": [
    "Recall that the data consists of 8×8 pixel images, meaning that they are 64-dimensional.\n",
    "To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "3wO8vkOJr83U",
    "outputId": "297e972f-36cc-405f-9cf5-d17d7a5254de"
   },
   "outputs": [],
   "source": [
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rU_zHvlr83W"
   },
   "source": [
    "We can now plot the first two principal components of each point to learn about the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "km_KF_9wr83W",
    "outputId": "c489adf3-69d8-4ea8-bf71-db610f8ae36b"
   },
   "outputs": [],
   "source": [
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('Accent', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "05.09-Principal-Component-Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
